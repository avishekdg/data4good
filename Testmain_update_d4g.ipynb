{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a0ef735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: predictionguard in c:\\users\\avish\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: tabulate>=0.8.10 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from predictionguard) (0.8.10)\n",
      "Requirement already satisfied: requests>=2.27.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from predictionguard) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests>=2.27.1->predictionguard) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests>=2.27.1->predictionguard) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests>=2.27.1->predictionguard) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests>=2.27.1->predictionguard) (2023.7.22)\n",
      "Requirement already satisfied: transformers in c:\\users\\avish\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/9a/06/e4ec2a321e57c03b7e9345d709d554a52c33760e5015fdff0919d9459af0/transformers-4.35.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.1 kB ? eta -:--:--\n",
      "     -------------------------------------  122.9/123.1 kB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 123.1/123.1 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/c3/29/0d9975fb739bdbefc73b6c23f335ea18e752fe6d2e91f3266a10dc8be140/tokenizers-0.14.1-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.14.1-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/da/33/f7437e23b0bb3f14014ab60de5948ca2b5187031e955e2db2fa872e35a3c/safetensors-0.4.0-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp311-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\avish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\avish\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/7.9 MB 11.2 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.0/7.9 MB 11.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.9/7.9 MB 15.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.9/7.9 MB 16.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.9/7.9 MB 17.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.6/7.9 MB 17.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.5/7.9 MB 17.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.6/7.9 MB 18.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/7.9 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 18.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 15.8 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.0-cp311-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 277.4/277.4 kB 16.7 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.14.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.0/2.2 MB 22.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.2 MB 17.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 15.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "   ---------------------------------------- 0.0/295.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 295.0/295.0 kB 9.2 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 2.1.1\n",
      "    Uninstalling transformers-2.1.1:\n",
      "      Successfully uninstalled transformers-2.1.1\n",
      "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n",
      "Enter your Prediction Guard access token: ········\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AutoModelForSequenceClassification' from 'transformers' (C:\\Users\\avish\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m transcript\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Define a function to classify the transcript type\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[0;32m     34\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmrm8488/t5-base-finetuned-medical-dialogues\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmrm8488/t5-base-finetuned-medical-dialogues\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'AutoModelForSequenceClassification' from 'transformers' (C:\\Users\\avish\\anaconda3\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Install the Prediction Guard client\n",
    "! pip install -U predictionguard\n",
    "! pip install transformers --upgrade\n",
    "\n",
    "# Python Imports\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "\n",
    "pg_access_token = getpass('Enter your Prediction Guard access token: ')\n",
    "os.environ['PREDICTIONGUARD_TOKEN'] = pg_access_token\n",
    "os.environ['PREDICTIONGUARD_URL'] = \"https://intel.predictionguard.com\"\n",
    "\n",
    "import predictionguard as pg\n",
    "\n",
    "# Transcripts\n",
    "with open('transcripts.json') as f:\n",
    "  transcripts = json.load(f)\n",
    "\n",
    "# Test data\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Define a function to remove random signs from the transcript\n",
    "def remove_random_signs(transcript):\n",
    "  transcript = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", transcript)\n",
    "  return transcript\n",
    "\n",
    "# Define a function to classify the transcript type\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-medical-dialogues\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/t5-base-finetuned-medical-dialogues\")\n",
    "\n",
    "def classify_transcript(transcript):\n",
    "  inputs = tokenizer(transcript, return_tensors=\"pt\")\n",
    "  outputs = model(**inputs)\n",
    "  predictions = outputs.logits.argmax(-1)\n",
    "  labels = [\"Doctor-Patient Conversation\", \"Doctor Dictation\"]\n",
    "  return labels[predictions[0]]\n",
    "\n",
    "# Define a function to create a prompt for each question\n",
    "def create_prompt(transcript, question, transcript_type):\n",
    "  if transcript_type == \"Doctor-Patient Conversation\":\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Extract the patient's condition from the below input.\n",
    "\n",
    "### Input:\n",
    "{transcript}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "  elif transcript_type == \"Doctor Dictation\":\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Extract the diagnosis and treatment from the below input.\n",
    "\n",
    "### Input:\n",
    "{transcript}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "  else:\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Answer the question based on the below input.\n",
    "\n",
    "### Input:\n",
    "{transcript}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "  return prompt\n",
    "\n",
    "# Define a function to use the LLM model to generate an answer\n",
    "def generate_answer(transcript, question):\n",
    "  # Remove random signs from the transcript\n",
    "  transcript = remove_random_signs(transcript)\n",
    "  # Classify the transcript type\n",
    "  transcript_type = classify_transcript(transcript)\n",
    "  # Create a prompt for the question\n",
    "  prompt = create_prompt(transcript, question, transcript_type)\n",
    "  # Use the Falcon-40B-Instruct model to generate an answer\n",
    "  result = pg.Completion.create(\n",
    "    model=\"Falcon-40B-Instruct\",\n",
    "    prompt=prompt\n",
    "  )\n",
    "  # Return the answer\n",
    "  return result['text']\n",
    "\n",
    "# Define a list to store the answers\n",
    "answers = []\n",
    "\n",
    "# Loop through the test data\n",
    "for _, row in test.iterrows():\n",
    "  # Get the transcript id and question from the row\n",
    "  transcript_id = row['Transcript']\n",
    "  question = row['Question']\n",
    "  # Get the transcript text from the transcripts\n",
    "  transcript = transcripts[str(transcript_id)]\n",
    "  # Generate an answer using the LLM model\n",
    "  answer = generate_answer(transcript, question)\n",
    "  # Append the answer to the list\n",
    "  answers.append([row['Id'], answer])\n",
    "\n",
    "# Create a dataframe with the answers\n",
    "df = pd.DataFrame(answers, columns=[\"Id\", \"Text\"])\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "df.to_csv(\"sample_submission_3.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73934d10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
