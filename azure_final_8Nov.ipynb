{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28abfc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "import time\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.ai.textanalytics import TextAnalyticsApiVersion\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "# Define the Azure Text Analytics and Translator credentials\n",
    "text_analytics_key = \"07e23bb5db3c47f294e5ab663a07d78c\"\n",
    "text_analytics_endpoint = \"https://azureapi-language.cognitiveservices.azure.com/\"\n",
    "translator_key = \"cfb87536adfa4c4cb30b5fc061f1bd62\"\n",
    "translator_endpoint = \"https://api.cognitive.microsofttranslator.com/\"\n",
    "\n",
    "# Create the Text Analytics and Translator clients\n",
    "text_analytics_client = TextAnalyticsClient(\n",
    "    endpoint=text_analytics_endpoint,\n",
    "    credential=AzureKeyCredential(text_analytics_key),\n",
    "    #api_version=TextAnalyticsApiVersion.V3_0_0 # Use the latest preview version for Text Analytics for health\n",
    ")\n",
    "translator_client = requests.Session()\n",
    "translator_client.headers.update({\n",
    "    'Ocp-Apim-Subscription-Key': translator_key,\n",
    "    'Ocp-Apim-Subscription-Region': \"eastus\",\n",
    "    'Content-type': 'application/json'\n",
    "})\n",
    "\n",
    "# Define the input and output files\n",
    "input_file = \"transcripts.json\"\n",
    "output_file = \"output_final_updeate.csv\"\n",
    "\n",
    "# Define the rate limit for the free tier (5 requests per second)\n",
    "rate_limit = 5\n",
    "\n",
    "# Define a helper function to translate a text to English\n",
    "def translate_to_english(text):\n",
    "    # Construct the request body\n",
    "    body = [{\n",
    "        'text': text\n",
    "    }]\n",
    "    # Send the request to the Translator service\n",
    "    response = translator_client.post(translator_endpoint + '/translate?api-version=3.0&to=en', json=body)\n",
    "    # Parse the response\n",
    "    result = response.json()\n",
    "    # Return the translated text\n",
    "    return result[0]['translations'][0]['text']\n",
    "\n",
    "# Open the input file and load the JSON data with error handling\n",
    "try:\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "except (IOError, json.JSONDecodeError) as e:\n",
    "    print(f'Error reading input file: {e}')\n",
    "    data = {}\n",
    "\n",
    "# Open the output file and create a CSV writer with error handling\n",
    "try:\n",
    "    with open(output_file, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Write the header row\n",
    "        writer.writerow(['id', 'text', 'language', 'key_phrases'])\n",
    "\n",
    "        # Loop through the data items\n",
    "        for id, text in data.items():\n",
    "            # Detect the language of the text\n",
    "            try:\n",
    "                response = text_analytics_client.detect_language(documents=[text])\n",
    "                language = response[0].primary_language.name\n",
    "            except HttpResponseError as e:\n",
    "                print(f'Error detecting language for id {id}: {e}')\n",
    "                language = 'Unknown'\n",
    "\n",
    "            # If the language is not English, translate the text to English\n",
    "            if language != 'English':\n",
    "                text = translate_to_english(text)\n",
    "\n",
    "            # Extract the key phrases from the text\n",
    "            try:\n",
    "                response = text_analytics_client.extract_key_phrases(documents=[text])\n",
    "                key_phrases = ', '.join(response[0].key_phrases)\n",
    "            except HttpResponseError as e:\n",
    "                print(f'Error extracting key phrases for id {id}: {e}')\n",
    "                key_phrases = ''\n",
    "\n",
    "            # Write the output row\n",
    "            try:\n",
    "                writer.writerow([id, text, language, key_phrases])\n",
    "            except UnicodeEncodeError as e:\n",
    "                print(f'Error writing row to output file for id {id}: {e}')\n",
    "\n",
    "            # Wait for 1/rate_limit seconds to avoid exceeding the free tier limit\n",
    "            time.sleep(1/rate_limit)\n",
    "except (IOError, csv.Error) as e:\n",
    "    print(f'Error writing to the output file: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa21c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning and saving to cleaned_output_file.csv is complete.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"output_final_updeate.csv\"\n",
    "output_file = \"cleaned_output_file.csv\"\n",
    "\n",
    "# Function to clean text by removing random signs\n",
    "def clean_text(text):\n",
    "    # Define regular expressions to match unwanted characters\n",
    "    unwanted_chars = r\"[^a-zA-Z0-9\\s]\"\n",
    "    clean_text = re.sub(unwanted_chars, \"\", text)\n",
    "    return clean_text\n",
    "\n",
    "# Read the input CSV file and create the cleaned data\n",
    "cleaned_data = []\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        id = row['id']\n",
    "        text = row['text']\n",
    "        cleaned_text = clean_text(text)\n",
    "        cleaned_data.append({'id': id, 'text': cleaned_text})\n",
    "\n",
    "# Write the cleaned data to the output CSV file\n",
    "with open(output_file, 'w', encoding='utf-8', newline='') as f:\n",
    "    fieldnames = ['id', 'text']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in cleaned_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Text cleaning and saving to {output_file} is complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfe7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
