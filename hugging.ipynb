{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c376fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\avish\\anaconda3\\lib\\site-packages (4.35.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\avish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\avish\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/7c/55/b3432f43d6d7fee999bb23a547820d74c48ec540f5f7842e41aa5d8d5f3a/datasets-2.14.6-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/b7/3a/74a609706ef4430fe6d041a3b8d209882c15440b695e373fe26d48c6f35c/xxhash-3.4.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/e7/41/96ac938770ba6e7d5ae1d8c9cafebac54b413549042c6260f0d0a6ec6622/multiprocess-0.70.15-py311-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (2023.3.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\avish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\avish\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Obtaining dependency information for dill<0.3.8,>=0.3.0 from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "   ---------------------------------------- 0.0/493.7 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 174.1/493.7 kB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 493.7/493.7 kB 6.2 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "   ---------------------------------------- 0.0/135.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 135.4/135.4 kB 7.8 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 115.3/115.3 kB ? eta 0:00:00\n",
      "Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl (29 kB)\n",
      "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "Successfully installed datasets-2.14.6 dill-0.3.7 multiprocess-0.70.15 xxhash-3.4.1\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.5 kB ? eta -:--:--\n",
      "     - -------------------------------------- 30.7/977.5 kB ? eta -:--:--\n",
      "     --- --------------------------------- 81.9/977.5 kB 919.0 kB/s eta 0:00:01\n",
      "     ------- ------------------------------ 204.8/977.5 kB 1.6 MB/s eta 0:00:01\n",
      "     ------------------------ ------------- 624.6/977.5 kB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  972.8/977.5 kB 5.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 977.5/977.5 kB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a40c0f383bf4d28a1e0b9cfc40e7909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avish\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\avish\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a501f0815c453181a2845c0ae2ae56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae567dd7ad05484a9a5d16ad89f73ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83dda129bbdf4f21b5744141befdb6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForQuestionAnswering requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Load the pre-trained tokenizer and model for question answering\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# You can choose any model from https://huggingface.co/models?filter=question-answering\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# For example, you can use bert-large-uncased-whole-word-masking-finetuned-squad\u001b[39;00m\n\u001b[0;32m     24\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-large-uncased-whole-word-masking-finetuned-squad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForQuestionAnswering\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-large-uncased-whole-word-masking-finetuned-squad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Create a question answering pipeline using the tokenizer and model\u001b[39;00m\n\u001b[0;32m     28\u001b[0m qa_pipeline \u001b[38;5;241m=\u001b[39m QuestionAnsweringPipeline(model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1251\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1251\u001b[0m requires_backends(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_backends)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1239\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1237\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModelForQuestionAnswering requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# Install the Hugging Face Transformers library and its dependencies\n",
    "! pip install -U transformers\n",
    "! pip install -U datasets\n",
    "! pip install -U sentencepiece\n",
    "\n",
    "# Import the necessary modules\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, QuestionAnsweringPipeline\n",
    "\n",
    "# Load the transcripts from the csv file named \"cleaned_output_file\"\n",
    "transcripts = pd.read_csv('cleaned_output_file.csv')\n",
    "\n",
    "# Convert the transcripts dataframe to a dictionary with id as the key and text as the value\n",
    "transcripts = transcripts.set_index('id')['text'].to_dict()\n",
    "\n",
    "# Load the test data from the csv file named \"test.csv\"\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Load the pre-trained tokenizer and model for question answering\n",
    "# You can choose any model from https://huggingface.co/models?filter=question-answering\n",
    "# For example, you can use bert-large-uncased-whole-word-masking-finetuned-squad\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "# Create a question answering pipeline using the tokenizer and model\n",
    "qa_pipeline = QuestionAnsweringPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Initialize an empty list to store the answers\n",
    "answers = []\n",
    "\n",
    "# Loop over all the rows in the test data\n",
    "for _, row in test.iterrows():\n",
    "  # Get the question and the transcript id from the row\n",
    "  question = row['Question']\n",
    "  transcript_id = row['Transcript']\n",
    "\n",
    "  # Get the transcript text from the transcripts dictionary\n",
    "  transcript = transcripts[transcript_id]\n",
    "\n",
    "  # Use the question answering pipeline to generate an answer based on the question and the transcript\n",
    "  answer = qa_pipeline(question=question, context=transcript)\n",
    "\n",
    "  # Append the answer and the question id to the answers list\n",
    "  answers.append([answer, row['Id']])\n",
    "\n",
    "# Convert the answers list to a pandas dataframe\n",
    "answers = pd.DataFrame(answers, columns=[\"Text\", \"Id\"])\n",
    "\n",
    "# Save the answers dataframe to a csv file\n",
    "answers.to_csv(\"sample_submission_8nov.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3e684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
