{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1016d12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/74/07/edce54779f5c3fe8ab8390eafad3d7c8190fce68f922a254ea77f4a94a99/torch-2.1.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torch-2.1.0-cp311-cp311-win_amd64.whl.metadata (25 kB)\n",
      "Collecting torchvision\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/20/ac/ab6f42af83349e679b03c9bb18354740c6b58b17dba329fb408730230584/torchvision-0.16.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torchvision-0.16.0-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Obtaining dependency information for torchaudio from https://files.pythonhosted.org/packages/11/30/715101782513f94c834ebe3afb9a29b0fae1121f64963db9d39fb80da53e/torchaudio-2.1.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading torchaudio-2.1.0-cp311-cp311-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\avish\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\avish\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\avish\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\avish\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\avish\\anaconda3\\lib\\site-packages (from torch) (2023.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\avish\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\avish\\anaconda3\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.1.0-cp311-cp311-win_amd64.whl (192.3 MB)\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/192.3 MB 6.3 MB/s eta 0:00:31\n",
      "   ---------------------------------------- 0.7/192.3 MB 7.4 MB/s eta 0:00:26\n",
      "   ---------------------------------------- 1.5/192.3 MB 11.6 MB/s eta 0:00:17\n",
      "    --------------------------------------- 2.5/192.3 MB 13.0 MB/s eta 0:00:15\n",
      "    --------------------------------------- 3.4/192.3 MB 15.3 MB/s eta 0:00:13\n",
      "    --------------------------------------- 4.1/192.3 MB 15.3 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 4.9/192.3 MB 15.6 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 5.9/192.3 MB 16.4 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 7.1/192.3 MB 17.4 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 7.9/192.3 MB 17.5 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 8.9/192.3 MB 17.7 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 9.7/192.3 MB 17.7 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 10.7/192.3 MB 19.3 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 11.7/192.3 MB 20.5 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 12.8/192.3 MB 20.5 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 13.9/192.3 MB 21.1 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 14.7/192.3 MB 21.8 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 16.0/192.3 MB 21.9 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 17.1/192.3 MB 21.8 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 18.2/192.3 MB 22.6 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 19.4/192.3 MB 22.6 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 20.6/192.3 MB 24.2 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 21.8/192.3 MB 23.4 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 22.9/192.3 MB 25.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 24.2/192.3 MB 24.3 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 25.2/192.3 MB 26.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 26.5/192.3 MB 26.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 27.5/192.3 MB 25.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 28.7/192.3 MB 25.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 29.6/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 30.7/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 31.7/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 32.7/192.3 MB 23.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 34.0/192.3 MB 23.4 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 35.4/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 36.6/192.3 MB 24.3 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 38.2/192.3 MB 25.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 39.2/192.3 MB 25.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 41.1/192.3 MB 28.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 42.4/192.3 MB 28.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 43.8/192.3 MB 29.7 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 44.7/192.3 MB 29.7 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 46.3/192.3 MB 31.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 47.8/192.3 MB 31.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 49.2/192.3 MB 29.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 50.5/192.3 MB 29.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 50.7/192.3 MB 27.3 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 52.6/192.3 MB 28.5 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 54.0/192.3 MB 28.5 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 55.2/192.3 MB 28.5 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 56.7/192.3 MB 28.5 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 58.0/192.3 MB 28.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 59.4/192.3 MB 28.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 60.7/192.3 MB 31.1 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 62.1/192.3 MB 29.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 63.6/192.3 MB 29.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 64.9/192.3 MB 29.7 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 66.3/192.3 MB 29.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 67.7/192.3 MB 29.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 69.0/192.3 MB 29.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 70.4/192.3 MB 29.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 71.7/192.3 MB 29.7 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 73.2/192.3 MB 29.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 74.6/192.3 MB 29.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 75.9/192.3 MB 29.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 77.3/192.3 MB 29.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 78.7/192.3 MB 29.7 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 79.9/192.3 MB 28.4 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 81.4/192.3 MB 29.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 82.9/192.3 MB 29.7 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 84.3/192.3 MB 29.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 85.7/192.3 MB 29.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 87.0/192.3 MB 29.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 88.4/192.3 MB 29.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 89.8/192.3 MB 29.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 91.1/192.3 MB 29.7 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 92.4/192.3 MB 28.4 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 93.9/192.3 MB 29.7 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 95.2/192.3 MB 29.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 96.6/192.3 MB 29.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 98.0/192.3 MB 31.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 99.3/192.3 MB 31.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 100.8/192.3 MB 29.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 102.1/192.3 MB 29.7 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 103.5/192.3 MB 29.7 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 104.9/192.3 MB 29.7 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 106.2/192.3 MB 29.7 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 107.6/192.3 MB 29.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 109.0/192.3 MB 29.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 110.2/192.3 MB 29.8 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 111.7/192.3 MB 29.8 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 113.1/192.3 MB 29.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 113.9/192.3 MB 29.8 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 115.5/192.3 MB 28.5 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 117.2/192.3 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 118.5/192.3 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 119.9/192.3 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 121.3/192.3 MB 29.8 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 122.7/192.3 MB 29.8 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 124.1/192.3 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 125.4/192.3 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 126.8/192.3 MB 29.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 128.1/192.3 MB 29.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 129.6/192.3 MB 29.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 131.0/192.3 MB 29.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 132.3/192.3 MB 29.8 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 133.8/192.3 MB 29.8 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 135.1/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 136.5/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 137.8/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 139.2/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 140.4/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 141.9/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 143.2/192.3 MB 28.5 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 144.6/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 145.9/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 147.3/192.3 MB 29.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 148.6/192.3 MB 29.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 149.8/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 151.3/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 152.6/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 154.1/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 155.4/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 156.8/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 158.1/192.3 MB 29.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 159.6/192.3 MB 29.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 160.9/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 162.3/192.3 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 163.4/192.3 MB 28.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 165.0/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 166.4/192.3 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 167.5/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 169.0/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 170.4/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 171.7/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 173.1/192.3 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 174.5/192.3 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 175.9/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 177.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 178.7/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 180.0/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 181.5/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 182.9/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 184.2/192.3 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 185.3/192.3 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 186.9/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  188.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  189.7/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  191.0/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.3 MB 28.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 192.3/192.3 MB 3.4 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.16.0-cp311-cp311-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.2/1.3 MB 39.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 20.2 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.1.0-cp311-cp311-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.3 MB 30.1 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.3 MB 13.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.3 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.3 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.3 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.3 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.8/2.3 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.3/2.3 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.3/2.3 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 5.5 MB/s eta 0:00:00\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.1.0 torchaudio-2.1.0 torchvision-0.16.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea38c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\avish\\anaconda3\\lib\\site-packages (4.35.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\avish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\avish\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: datasets in c:\\users\\avish\\anaconda3\\lib\\site-packages (2.14.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (2023.3.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\avish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\avish\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\avish\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\avish\\anaconda3\\lib\\site-packages (0.1.99)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2da8e5869e349379cae42815ff1e5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avish\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\avish\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe32f66e5b642f1af6053fa312c4d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609f0e2416904290aa61a2c43f6788d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28e98298d92464fa6a206ee0c034561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191246883d72457496d073241a0d94e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efd9e387b684f309ead65edb5c6e047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Use the question answering pipeline to generate an answer based on the question and the transcript\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Add a try-except block to catch and handle any errors or exceptions\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m   answer \u001b[38;5;241m=\u001b[39m qa_pipeline(question\u001b[38;5;241m=\u001b[39mquestion, context\u001b[38;5;241m=\u001b[39mtranscript)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     46\u001b[0m   \u001b[38;5;66;03m# Print the error message and the question id to the console\u001b[39;00m\n\u001b[0;32m     47\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for question id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:393\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_parser(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1132\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[1;32m-> 1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1134\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1135\u001b[0m                 [inputs], num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1136\u001b[0m             )\n\u001b[0;32m   1137\u001b[0m         )\n\u001b[0;32m   1138\u001b[0m     )\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:266\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[1;32m--> 266\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:520\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline._forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    519\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m: example, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1500\u001b[0m, in \u001b[0;36mRobertaForQuestionAnswering.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1488\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1500\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta(\n\u001b[0;32m   1501\u001b[0m     input_ids,\n\u001b[0;32m   1502\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1503\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1504\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1505\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1506\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1507\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1508\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1509\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1510\u001b[0m )\n\u001b[0;32m   1512\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1514\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:835\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    826\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    828\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    829\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    830\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    833\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    834\u001b[0m )\n\u001b[1;32m--> 835\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    836\u001b[0m     embedding_output,\n\u001b[0;32m    837\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m    838\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    839\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    840\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m    841\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    842\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    843\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    844\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    845\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    846\u001b[0m )\n\u001b[0;32m    847\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    848\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    513\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    514\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    515\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m         output_attentions,\n\u001b[0;32m    522\u001b[0m     )\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    525\u001b[0m         hidden_states,\n\u001b[0;32m    526\u001b[0m         attention_mask,\n\u001b[0;32m    527\u001b[0m         layer_head_mask,\n\u001b[0;32m    528\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    529\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    530\u001b[0m         past_key_value,\n\u001b[0;32m    531\u001b[0m         output_attentions,\n\u001b[0;32m    532\u001b[0m     )\n\u001b[0;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    403\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    412\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 413\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    414\u001b[0m         hidden_states,\n\u001b[0;32m    415\u001b[0m         attention_mask,\n\u001b[0;32m    416\u001b[0m         head_mask,\n\u001b[0;32m    417\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    418\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    419\u001b[0m     )\n\u001b[0;32m    420\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    332\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    338\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 340\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    341\u001b[0m         hidden_states,\n\u001b[0;32m    342\u001b[0m         attention_mask,\n\u001b[0;32m    343\u001b[0m         head_mask,\n\u001b[0;32m    344\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    345\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    346\u001b[0m         past_key_value,\n\u001b[0;32m    347\u001b[0m         output_attentions,\n\u001b[0;32m    348\u001b[0m     )\n\u001b[0;32m    349\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    350\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:219\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    217\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m1\u001b[39m], value_layer], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 219\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[0;32m    220\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(hidden_states))\n\u001b[0;32m    222\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Install the Hugging Face Transformers library and its dependencies\n",
    "! pip install -U transformers\n",
    "! pip install -U datasets\n",
    "! pip install -U sentencepiece\n",
    "\n",
    "# Import the necessary modules\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, QuestionAnsweringPipeline\n",
    "\n",
    "# Load the transcripts from the csv file named \"cleaned_output_file\"\n",
    "transcripts = pd.read_csv('cleaned_output_file.csv')\n",
    "\n",
    "# Convert the transcripts dataframe to a dictionary with id as the key and text as the value\n",
    "transcripts = transcripts.set_index('id')['text'].to_dict()\n",
    "\n",
    "# Load the test data from the csv file named \"test.csv\"\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Load the pre-trained tokenizer and model for question answering\n",
    "# Use deepset/roberta-base-squad2 instead of bert-large-uncased-whole-word-masking-finetuned-squad\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# Create a question answering pipeline using the tokenizer and model\n",
    "qa_pipeline = QuestionAnsweringPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Initialize an empty list to store the answers\n",
    "answers = []\n",
    "\n",
    "# Loop over all the rows in the test data\n",
    "for _, row in test.iterrows():\n",
    "  # Get the question and the transcript id from the row\n",
    "  question = row['Question']\n",
    "  transcript_id = row['Transcript']\n",
    "\n",
    "  # Get the transcript text from the transcripts dictionary\n",
    "  transcript = transcripts[transcript_id]\n",
    "\n",
    "  # Use the question answering pipeline to generate an answer based on the question and the transcript\n",
    "  # Add a try-except block to catch and handle any errors or exceptions\n",
    "  try:\n",
    "    answer = qa_pipeline(question=question, context=transcript)\n",
    "  except Exception as e:\n",
    "    # Print the error message and the question id to the console\n",
    "    print(f\"Error: {e} for question id {row['Id']}\")\n",
    "    # Set the answer to None\n",
    "    answer = None\n",
    "\n",
    "  # Append the answer and the question id to the answers list\n",
    "  answers.append([answer, row['Id']])\n",
    "\n",
    "# Convert the answers list to a pandas dataframe\n",
    "# Add the header, index and index_label arguments to the to_csv method\n",
    "answers = pd.DataFrame(answers, columns=[\"Text\", \"Id\"])\n",
    "answers.to_csv(\"sample_submission_9nov_up3.csv\", index=True, index_label=\"Id\", header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5d409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Append the answer and the question id to the answers list\n",
    "  answers.append([answer, row['Id']])\n",
    "\n",
    "# Convert the answers list to a pandas dataframe\n",
    "# Add the header, index and index_label arguments to the to_csv method\n",
    "answers = pd.DataFrame(answers, columns=[\"Text\", \"Id\"])\n",
    "answers.to_csv(\"sample_submission_9nov_up3.csv\", index=True, index_label=\"Id\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339218b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
